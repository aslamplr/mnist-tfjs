{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Model_for_web.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "wwA8UssSmJuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST Handwritten digit classifier in browser\n",
        "\n",
        "This notebook creates, train and export a model trained in MNIST handwritten digit classifying task, to be imported by tfjs in browser.\n"
      ]
    },
    {
      "metadata": {
        "id": "BsJ7LGe-mmoS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "install `tensorflowjs`, which is only required in this notebook for saving the model compatible with `tensorflowjs`"
      ]
    },
    {
      "metadata": {
        "id": "855JDkg9jBMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1092
        },
        "outputId": "e710ff75-bda6-41d3-8dde-4e2df21b58dd"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading https://files.pythonhosted.org/packages/79/29/35e1aa467436ff46b98df65a08c49faaedb3429e1c512d1d90fe308040a0/tensorflowjs-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Collecting tf-nightly-2.0-preview>=2.0.0.dev20190304 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/98/4ed54e50175bec8db846427d12b60c6dddc5083d7f283bcb7d18ee5dd9c1/tf_nightly_2.0_preview-2.0.0.dev20190412-cp36-cp36m-manylinux1_x86_64.whl (86.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 86.5MB 448kB/s \n",
            "\u001b[?25hCollecting numpy==1.15.1 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/94/7049fed8373c52839c8cde619acaf2c9b83082b935e5aa8c0fa27a4a8bcc/numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.2.4)\n",
            "Collecting tensorflow-hub==0.3.0 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/f0/3a3ced04c8359e562f1b91918d9bde797c8a916fcfeddc8dc5d673d1be20/tensorflow_hub-0.3.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 21.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/8b/471899f8c90bd2c91ee6f52f66665b692457cf29b4ff422e2245e9095b8f/tensorflow_estimator_2.0_preview-1.14.0.dev2019041200-py2.py3-none-any.whl (356kB)\n",
            "\u001b[K    100% |████████████████████████████████| 358kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.33.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.0.7)\n",
            "Collecting wrapt>=1.11.1 (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (3.7.1)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c7/e6b9a0a416303c76b4d557d170617e8fcce2e33baedc3b02f4994e736ef1/tb_nightly-1.14.0a20190412-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.0.9)\n",
            "Collecting google-pasta>=0.1.2 (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (40.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (3.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, wrapt, numpy, tb-nightly, google-pasta, tf-nightly-2.0-preview, tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: wrapt 1.10.11\n",
            "    Uninstalling wrapt-1.10.11:\n",
            "      Successfully uninstalled wrapt-1.10.11\n",
            "  Found existing installation: numpy 1.16.2\n",
            "    Uninstalling numpy-1.16.2:\n",
            "      Successfully uninstalled numpy-1.16.2\n",
            "  Found existing installation: tensorflow-hub 0.4.0\n",
            "    Uninstalling tensorflow-hub-0.4.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.4.0\n",
            "Successfully installed google-pasta-0.1.5 numpy-1.15.1 tb-nightly-1.14.0a20190412 tensorflow-estimator-2.0-preview-1.14.0.dev2019041200 tensorflow-hub-0.3.0 tensorflowjs-1.0.1 tf-nightly-2.0-preview-2.0.0.dev20190412 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "rjf9ZYItm2Yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Packages required for the notebook"
      ]
    },
    {
      "metadata": {
        "id": "twu1gnJ0h8_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bf450cdf-89bb-418d-d1c3-b464465a86e4"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs\n",
        "from tensorflow import keras\n",
        "\n",
        "print('tensorflow', tf.__version__)\n",
        "print('tensorflowjs', tfjs.__version__)\n",
        "print('keras', keras.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow 2.0.0-dev20190412\n",
            "tensorflowjs 1.0.1\n",
            "keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZeTyjXn0ny-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ]
    },
    {
      "metadata": {
        "id": "DkSnKIQNnyVj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGE_WIDTH = 28\n",
        "IMAGE_HEIGHT = 28\n",
        "IMAGE_CHANNELS = 1\n",
        "\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15udzoCYnMDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The model\n",
        "\n",
        "Let's create the model"
      ]
    },
    {
      "metadata": {
        "id": "ktSJlZx2nh4d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getModel():\n",
        "    model = keras.Sequential();\n",
        "    \n",
        "    # In the first layer of out convolutional neural network we have\n",
        "    # to specify the input shape. Then we specify some paramaters for\n",
        "    # the convolution operation that takes place in this layer.\n",
        "    model.add(keras.layers.Conv2D(\n",
        "        input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),\n",
        "        kernel_size=5,\n",
        "        filters=8,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        kernel_initializer='VarianceScaling'\n",
        "    ))\n",
        "    \n",
        "    # The MaxPooling layer acts as a sort of downsampling using max values\n",
        "    # in a region instead of averaging.\n",
        "    model.add(keras.layers.MaxPool2D(\n",
        "        pool_size=(2,2), \n",
        "        strides=(2,2)\n",
        "    ))\n",
        "    \n",
        "    # Repeat another Conv2D + MazPooling stack.\n",
        "    # Note that we have more filters in the convolution.\n",
        "    model.add(keras.layers.Conv2D(\n",
        "        kernel_size=5,\n",
        "        filters=16,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        kernel_initializer='VarianceScaling'\n",
        "    ))\n",
        "    \n",
        "    model.add(keras.layers.MaxPool2D(\n",
        "        pool_size=(2,2),\n",
        "        strides=(2,2)\n",
        "    ))\n",
        "    \n",
        "    \n",
        "    # Now we flatten the output from the 2D filters into a 1D vector to prepare\n",
        "    # it for input into our last layer. This is common practice when feeding\n",
        "    # higher dimensional data to a final classification output layer.\n",
        "    model.add(keras.layers.Flatten())\n",
        "    \n",
        "    \n",
        "    # Our last layer is a dense layer which has 10 output units, one for each\n",
        "    # output class (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
        "    NUM_OUTPUT_CLASSES = 10;\n",
        "    model.add(keras.layers.Dense(\n",
        "        units=NUM_OUTPUT_CLASSES,\n",
        "        kernel_initializer='VarianceScaling',\n",
        "        activation='softmax'\n",
        "    ))\n",
        "    \n",
        "    \n",
        "    # Choose an optimizer, loss function and accuracy matric,\n",
        "    # then compile and return the model\n",
        "    optimizer = keras.optimizers.Adam();\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "  \n",
        "model = getModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAaB2Hxf495N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ]
    },
    {
      "metadata": {
        "id": "DrMF8vYtnKGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "e31b624f-cd80-45a2-8c06-af875cf0aaa0"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def train(model, data, epochs):\n",
        "  BATCH_SIZE = 512\n",
        "  NUM_TRAIN_SAMPLES = 60000\n",
        "  NUM_TEST_SAMPLES = 10000\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  \n",
        "  x_train = x_train.reshape((NUM_TRAIN_SAMPLES, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n",
        "  x_test = x_test.reshape((NUM_TEST_SAMPLES, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n",
        "  \n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "  \n",
        "  model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=NUM_EPOCHS,\n",
        "      validation_data=(x_test, y_test),\n",
        "      shuffle=True,\n",
        "      callbacks=None\n",
        "  )\n",
        "  \n",
        "\n",
        "train(model, mnist, NUM_EPOCHS)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 22s 363us/sample - loss: 11.4702 - accuracy: 0.5888 - val_loss: 1.7214 - val_accuracy: 0.8089\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 22s 363us/sample - loss: 1.2803 - accuracy: 0.8426 - val_loss: 0.8812 - val_accuracy: 0.8784\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 22s 363us/sample - loss: 0.7405 - accuracy: 0.8901 - val_loss: 0.6032 - val_accuracy: 0.9054\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 22s 364us/sample - loss: 0.5202 - accuracy: 0.9094 - val_loss: 0.4665 - val_accuracy: 0.9193\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 22s 362us/sample - loss: 0.3927 - accuracy: 0.9243 - val_loss: 0.3720 - val_accuracy: 0.9283\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 22s 362us/sample - loss: 0.3157 - accuracy: 0.9339 - val_loss: 0.3056 - val_accuracy: 0.9352\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 22s 362us/sample - loss: 0.2597 - accuracy: 0.9414 - val_loss: 0.2630 - val_accuracy: 0.9408\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 22s 362us/sample - loss: 0.2224 - accuracy: 0.9470 - val_loss: 0.2300 - val_accuracy: 0.9461\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 22s 363us/sample - loss: 0.1950 - accuracy: 0.9529 - val_loss: 0.2063 - val_accuracy: 0.9493\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 22s 363us/sample - loss: 0.1707 - accuracy: 0.9568 - val_loss: 0.1920 - val_accuracy: 0.9529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "50JLVFzAC_lg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the following code to authenticate/authorize your Google Drive for the purpose of mounting in this virtual machine\n",
        "### we will use this to save the trained model to your GDrive"
      ]
    },
    {
      "metadata": {
        "id": "ftHask3MBwdI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "cf2667fd-941a-49f1-fd63-0da25636f5fc"
      },
      "cell_type": "code",
      "source": [
        "# Run this to authenticate/authorize your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LLx_2gZiDQI9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save the model"
      ]
    },
    {
      "metadata": {
        "id": "Ii62I4m-jiFH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_PATH_GDRIVE = '/content/gdrive/My Drive/mnist_cnn_tfjs'\n",
        "tfjs.converters.save_keras_model(model, MODEL_SAVE_PATH_GDRIVE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBS1CRtsDSGn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The trained model is now available in your Google Drive in the location `My Drive/mnist_cnn_tfjs`"
      ]
    }
  ]
}